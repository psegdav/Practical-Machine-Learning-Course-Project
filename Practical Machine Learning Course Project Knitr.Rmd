---
title: "Practical Machine Learning Course Project 1"
author: "psegdav"
date: "2/24/2021"
output:
  word_document: default
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# I. Introduction

This is the Course Project for Practical Machine Learning Course of Johns Hopkins University on Coursera. 

Due to devices as Nike FuelBand, Fitbit, and Jawbone Up, personal activity data can be collected in an inexpensive way. These are part of the quantified self movement devices. These devices allow to quantify how much of a particular activity the person does, however it is rarely quantified how well does cavity is done. 

In this course project we will use data from accelerometer on the belt, forearm, arm, and dumbell of 6 participants.  Each of the participants was asked to perform barbell lift in a proper way and incorrectly, in five different ways. The training and testing sets can be found in https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv and https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv, respectively. For more information regarding the data, please visit: http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har.

The goal is to predict the manner in which the participant did the exercise. This is shown in the "classe" variable within the train set. This will be done by creating a prediction model to predict 20 different test cases. 

# II. Data Analysis

```{r}
library(knitr)
library(caret)
library(rpart)
library(randomForest)
library(gbm)

train <- read.csv("pml-training.csv")
test <- read.csv("pml-testing.csv")

dim(train); dim(test)
```

We can see that the training set has 19622 observations and the testing set has 20 observations.  Once we examine the each data set to a major extent, we can identify many NA values, these values are not useful for out prediction models and therefore we will now clean the data by eliminating the NA values.The clean training data set will be ctrain 

```{r}
ctrain <- train[,colMeans(is.na(train))<0.9]
```

Once the NA values have been removed, we will remove the values with a near zero variance.

```{r}
cctrain <- nearZeroVar(ctrain)
ctrain <- ctrain[,-cctrain]

dim(ctrain)
```

The ctrain (clean training data set) will now be split, in order to have a validation data set (validation) and a training data set (utrain). 

```{r}
div_ctrain <- createDataPartition(y=ctrain$classe, p=0.7, list=FALSE)
validation <- ctrain[-div_ctrain,]
utrain <- ctrain[div_ctrain,]
```

# III. Model Creation and Testing

We will set up a control for the training.

```{r}
ctrl <- trainControl(method="cv", number=5)
```

We will start by creating a random forest model, as it is highly accurate. 

```{r}
mod_rf <- train(classe ~ ., method="rf", data=utrain, trControl= ctrl, allowParallel=T)

mod_rf

```

To also have a more visual model, we will create a decision tree model. 

```{r}
mod_dt <- train(classe ~ ., method="rpart", data=utrain)

mod_dt

```
## IV. Accuracy on validation and training set 

We will now  revise the accuracy of Random Forests and Decision Trees on the validation data set.

```{r}
pred_rf <- predict(mod_rf, validation)
cmatrix_rf <- confusionMatrix(pred_rf, factor(validation$classe))
cmatrix_rf

```

```{r}
pred_dt <- predict(mod_dt, validation)
cmatrix_dt <- confusionMatrix(pred_dt, factor(validation$classe))
cmatrix_dt
```


## V. Results & Conclusions
Based on the analyses made, the Random Forest is the best model, as it has an accuracy of 1 and a no information rate of 0.28. The Decision Tree model had an accuracy of 0.66 and a no information rate of 0.28. Due to the accuracy, it can be concluded that the Random Forest model is the most appropriate to make predictions on the test set. 

```{r}
pred_test <- predict(mod_rf, test)
pred_test

```

## V. Appendix

```{r}
plot(mod_rf, main="Random Forest")
```
```{r}
plot(mod_dt, main="Decision Tree")
```
```{r}
plot(pred_test, main="Test Prediction")
```